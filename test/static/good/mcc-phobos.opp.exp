File "mcc-phobos.mly", line 228, characters 33-41:
Warning: the token TokFloat is unused.
%{
open Symbol
open Opname
open Mp_num
open Refiner.Refiner.TermType
open Refiner.Refiner.Term
open Phobos_type
open Phobos_constants
open Phobos_marshal
open Phobos_parse_state
open Phobos_exn
open Phobos_util
open Phobos_rewrite
open Phobos_builtin

(*
 * This is the name of the module.
 * It is only used in the opname of a product term.
 *)
let module_name = ref ""

(*
 * There are terms that are private to a module.
 * If they are ever show up outside of the parser,
 * something is wrong.
 *)
let privates_module = "@"

(*
 * Support for module names.
 *)
let term_names = ref StringTable.empty

(*
 * Add each built-in term to the private list.
 *)
let _ =
   List.iter (fun s ->
      term_names := StringTable.add !term_names s (privates_module, -1)) (snd (List.hd built_in_terms))

let new_terms mdl decls =
   List.iter (fun ((id, pos), subterm_count) ->
      if StringTable.mem !term_names id then begin
         let mod_name, stc = StringTable.find !term_names id in
            Printf.eprintf "warning: %s : %s{?} has already been defined in another module [as %s{%d} in %s]\n" (**)
               (string_of_pos pos) id id stc mod_name
      end;
      if !Fir_state.debug_phobos then
         Format.print_string (Printf.sprintf "Adding term [%s]\n" id);
      term_names := StringTable.add !term_names id (mdl, subterm_count)) decls

let term_name_of (id, pos) =
   try
      let mod_name, subterm_count = StringTable.find !term_names id in
         if mod_name = privates_module then
            [id]
         else
            [id; mod_name]
   with
      Not_found ->
         raise (PhobosException (pos, Printf.sprintf "undefined term [%s]" id))

type param_type =
   TyString
 | TyNum
 | TyToken
 | TyLevel
 | TyVar

let pho_make_term id_pos_list params bterms =
   let opname =
      if List.length id_pos_list > 1 then
         List.map fst id_pos_list
      else
         term_name_of (List.hd id_pos_list)
   in
      mk_term (mk_op (make_opname opname) params) bterms

let pho_make_var_term (id, pos) =
   mk_var_term id

let pho_make_bterm id_pos_list pterm =
   let vars = List.map fst id_pos_list in
      mk_bterm vars (fst pterm)

let pho_make_unique_var_term = unique_var_term
let pho_make_token_term = token_term
let pho_make_prod_term = prod_term

let pho_make_number_term num =
   let param = make_param (Number (num_of_int (fst num))) in
      mk_term (mk_op (make_opname ["number"; "Itt_int_base"]) [param]) []

let rec make_rules (id, pos) = function
   head :: rest ->
      ((id, pos), head) :: make_rules (id, pos) rest
 | [] ->
      []

let make_rules head productions =
   List.rev (make_rules head productions)

let production_of_shorthand id_list opt_prec result =
   let from =
      List.map (fun id ->
         pho_make_unique_var_term id, snd id) id_list
   in
      id_list, opt_prec, [(from, result)]

let insert_rewrite_if_needed id_list = function
   [] ->
      let from_terms =
         List.map (fun (s, pos) ->
            pho_make_unique_var_term (s, pos)) id_list
      in
      let from = List.map2 (fun term (s, pos) -> term, pos) from_terms id_list in
      (match id_list with
         [s, pos] ->
               [from, List.hd from]
       | _ ->
            (* We use a bogus position for the result *)
            let result_pos = ("<prod>", 0, 0, 0, 0) in
            let result = pho_make_prod_term from_terms, result_pos in
               [from, result])
 | a ->
      a

let process_includes includes =
   List.iter (fun s ->
      if !Fir_state.debug_phobos then
         Format.print_string (Printf.sprintf "Loading %s..." s);
      let gst, _, _, _ = load_grammar s in
      if !Fir_state.debug_phobos then
         Format.print_string "done\n";
      let termsets = gst.grammar_termsets in
         List.iter (fun term_option_list ->
            List.iter (fun term_option ->
               if !Fir_state.debug_phobos then
                  Format.print_string ".";
               match term_option with
                  To_extend (mdl, decls) ->
                     new_terms mdl decls) term_option_list) termsets) includes;
      if !Fir_state.debug_phobos then
         Format.print_string "\nDone processing includes\n"

let include_built_in (s, pos) =
   let new_terms, found =
      List.fold_left (fun (new_terms, found) (mdl, terms) ->
         if s = mdl then
            new_terms @ terms, true
         else
            new_terms, found) ([], false) built_in_terms
   in
   if not found then
      raise (PhobosException (pos, Printf.sprintf (**)
         "Invalid internal module [%s]" s))
   else
      List.iter (fun term ->
         term_names := StringTable.add !term_names term (s, -1)) new_terms

%}
%start main
%token <Phobos_type.pos> TokArrow
%token <Phobos_type.pos> TokBang
%token <Phobos_type.pos> TokColon
%token <Phobos_type.pos> TokComma
%token <Phobos_type.pos> TokDeclare
%token <Phobos_type.pos> TokDot
%token <Phobos_type.pos> TokDoubledArrow
%token TokEof
%token <Phobos_type.pos> TokEq
%token <Phobos_type.pos> TokExtend
%token <Phobos_type.pos> TokFirst
%token <float * Phobos_type.pos> TokFloat
%token <Phobos_type.pos> TokGrammar
%token <string * Phobos_type.pos> TokId
%token <Phobos_type.pos> TokIgnore
%token <Phobos_type.pos> TokInclude
%token <Phobos_type.pos> TokInline
%token <int * Phobos_type.pos> TokInt
%token <Phobos_type.pos> TokLeftAssoc
%token <Phobos_type.pos> TokLeftBrace
%token <Phobos_type.pos> TokLeftBrack
%token <Phobos_type.pos> TokLongest
%token <Phobos_type.pos> TokModule
%token <Phobos_type.pos> TokNonAssoc
%token <string * Phobos_type.pos> TokOption
%token <Phobos_type.pos> TokPipe
%token <Phobos_type.pos> TokPrec
%token <Phobos_type.pos> TokQuestionMark
%token <string * Phobos_type.pos> TokQuotedId
%token <Phobos_type.pos> TokRewrites
%token <Phobos_type.pos> TokRightAssoc
%token <Phobos_type.pos> TokRightBrace
%token <Phobos_type.pos> TokRightBrack
%token <Phobos_type.pos> TokRuleEq
%token <Phobos_type.pos> TokSemi
%token <Phobos_type.pos> TokStart
%token <string * Phobos_type.pos> TokString
%token <Phobos_type.pos> TokTerms
%token <Phobos_type.pos> TokTokens
%type <Phobos_type.mp_pre_rewrite list * Phobos_type.pos> body
%type <Phobos_type.pre_rule list * Phobos_type.goption list> grammar
%type <Phobos_type.phobos_parser_return_type> main
%type <Phobos_type.pre_rule list> rule
%type <Phobos_type.pre_rule list> rule_list
%type <Phobos_type.pre_rule list> rules
%type <Phobos_type.mp_pre_term> simple_term
%type <Phobos_type.mp_pre_rewrite list> term_match_list
%%

main:
  _1 = module_name _2 = includes _3 = term_sections _4 = opt_preamble _5 = lexer _6 = opt_assocs _7 = grammar _8 = opt_rewrites_section _9 = opt_inline_forms _10 = TokEof
    {                              ( { phobos_module_name = _1;
                                  phobos_includes = _2;
                                  phobos_termsets = _3;
                                  phobos_local_rewrites = _4;
                                  phobos_lexer_info = _5;
                                  phobos_assoc_info = _6;
                                  phobos_grammar_info = _7;
                                  phobos_post_rewrites = _8;
                                  phobos_inline_forms = _9
                                }
                              )}

module_name:
  _1 = TokModule _2 = identifier
    {                              ( module_name := fst _2; fst _2 )}

includes:
  
    {                              ( [] )}
| _1 = include_list_rev
    {                              ( let mdl_names = List.flatten (List.rev _1) in
                                   process_includes mdl_names;
                                   mdl_names
                              )}

include_list_rev:
  _1 = include_list_rev _2 = include_item
    {                              ( _2 :: _1 )}
| _1 = include_item
    {                              ( [_1] )}

include_item:
  _1 = TokInclude _2 = string_list
    {                              ( _2 )}
| _1 = TokInclude _2 = identifier
    {                              ( include_built_in _2; [] )}

string_list:
  _1 = string_list_rev
    {                              ( List.rev _1 )}

string_list_rev:
  _1 = string_list _2 = TokString
    {                              ( (fst _2) :: _1 )}
| _1 = TokString
    {                              ( [fst _1] )}

term_sections:
  _1 = term_section_list
    {                              ( List.rev _1 )}

term_section_list:
  _1 = term_section
    {                              ( [_1] )}
| _1 = term_section_list _2 = term_section
    {                              ( _2 :: _1 )}

term_section:
  _1 = TokTerms _2 = term_options
    {                              ( _2 )}

term_options:
  _1 = term_option_list
    {                              ( List.rev _1 )}

term_option_list:
  _1 = term_option
    {                              ( [_1] )}
| _1 = term_option_list _2 = term_option
    {                              ( _2 :: _1 )}

term_option:
  _1 = TokExtend _2 = TokString _3 = TokLeftBrace _4 = opt_term_declarations _5 = TokRightBrace
    {                              ( new_terms (fst _2) _4;
                                To_extend (fst _2, _4)
                              )}
| _1 = TokOption
    {                              ( raise (ParseError (snd _1, string_add ["Invalid option \""; fst _1; "\""])) )}

opt_preamble:
  
    {                              ( [] )}
| _1 = preamble
    {                              ( _1 )}

preamble:
  _1 = TokLeftBrace _2 = new_rewrites _3 = TokRightBrace
    {                              ( _2 )}

lexer:
  _1 = TokTokens _2 = opt_lexer_options _3 = TokLeftBrace _4 = tokens _5 = TokRightBrace
    {                              ( _4, _2 )}

opt_lexer_options:
  
    {                              ( [] )}
| _1 = lexer_option_list
    {                              ( List.rev _1 )}

lexer_option_list:
  _1 = lexer_option_list _2 = lexer_option
    {                              ( _2 :: _1 )}
| _1 = lexer_option
    {                              ( [_1] )}

lexer_option:
  _1 = TokLongest
    {                              ( Lo_longest )}
| _1 = TokFirst
    {                              ( Lo_first )}
| _1 = TokOption
    {                              ( raise (ParseError (snd _1, string_add ["Invalid option \""; fst _1; "\""])) )}

tokens:
  _1 = token_list
    {                              ( List.rev _1 )}

token_list:
  _1 = token_list _2 = token
    {                              ( _2 :: _1 )}
| _1 = token
    {                              ( [_1] )}

token:
  _1 = TokId _2 = TokEq _3 = TokString _4 = token_body
    {                              ( false, fst _1, fst _3, fst _4 )}
| _1 = TokIgnore _2 = TokId _3 = TokEq _4 = TokString _5 = token_body
    {                              ( true, fst _2, fst _4, fst _5 )}

identifiers:
  _1 = rev_identifier_list
    {                              ( List.rev _1 )}

rev_identifier_list:
  _1 = identifier
    {                              ( [_1] )}
| _1 = rev_identifier_list _2 = identifier
    {                              ( _2 :: _1 )}

identifiers_with_comma:
  _1 = rev_identifier_list_with_comma
    {                              ( List.rev _1 )}

rev_identifier_list_with_comma:
  _1 = identifier
    {                              ( [_1] )}
| _1 = rev_identifier_list_with_comma _2 = TokComma _3 = identifier
    {                              ( _3 :: _1 )}

identifier:
  _1 = TokId
    {                              ( _1 )}

opt_assocs:
  
    {                              ( [] )}
| _1 = assoc_list
    {                              ( List.rev _1 )}

assoc_list:
  _1 = assoc_list _2 = assoc
    {                              ( _2 :: _1 )}
| _1 = assoc
    {                              ( [_1] )}

assoc:
  _1 = TokNonAssoc _2 = identifiers
    {                              ( Dir_nonassoc _2 )}
| _1 = TokLeftAssoc _2 = identifiers
    {                              ( Dir_leftassoc _2 )}
| _1 = TokRightAssoc _2 = identifiers
    {                              ( Dir_rightassoc _2 )}

grammar:
  _1 = TokGrammar _2 = opt_grammar_options _3 = TokLeftBrace _4 = rules _5 = TokRightBrace
    {                              ( _4, _2 )}

opt_grammar_options:
  
    {                              ( [] )}
| _1 = grammar_option_list
    {                              ( List.rev _1 )}

grammar_option_list:
  _1 = grammar_option_list _2 = grammar_option
    {                              ( _2 :: _1 )}
| _1 = grammar_option
    {                              ( [_1] )}

grammar_option:
  _1 = TokStart _2 = identifier
    {                              ( Go_start (fst _2) )}

rules:
  _1 = rule_list
    {                              ( _1 )}

rule_list:
  _1 = rule_list _2 = rule
    {                              ( _1 @ _2 )}
| _1 = rule
    {                              ( _1 )}

rule:
  _1 = TokId _2 = TokRuleEq _3 = productions
    {                              ( make_rules _1 _3 )}

productions:
  _1 = prod_body _2 = prods
    {                              ( _1 :: _2 )}
| _1 = prod_body
    {                              ( [_1] )}

opt_prec:
  
    {                              ( None )}
| _1 = TokPrec _2 = TokId
    {                              ( Some _2 )}

prods:
  _1 = prod_list
    {                              ( List.rev _1 )}

prod_list:
  _1 = prod_list _2 = prod_list_prim
    {                              ( _2 :: _1 )}
| _1 = prod_list_prim
    {                              ( [_1] )}

prod_list_prim:
  _1 = TokPipe _2 = prod_body
    {                              ( _2 )}

prod_body:
  _1 = identifiers _2 = opt_prec _3 = body
    {                              ( _1, _2, insert_rewrite_if_needed _1 (fst _3) )}
| _1 = identifiers _2 = opt_prec _3 = TokDoubledArrow _4 = simple_term
    {                              ( production_of_shorthand _1 _2 _4 )}

token_body:
  _1 = TokLeftBrace _2 = token_term_matches _3 = TokRightBrace
    {                              ( _2, union_pos _1 _3 )}

body:
  _1 = TokLeftBrace _2 = opt_term_matches _3 = TokRightBrace
    {                              ( _2, union_pos _1 _3 )}

new_rewrites:
  
    {                              ( [] )}
| _1 = new_rewrite_list
    {                              ( List.rev _1 )}

new_rewrite_list:
  _1 = new_rewrite_list _2 = new_rewrite
    {                              ( _2 :: _1 )}
| _1 = new_rewrite
    {                              ( [_1] )}

new_rewrite:
  _1 = from _2 = TokArrow _3 = simple_term
    {                              ( _1, _3 )}

token_term_matches:
  
    {                              ( let term = pho_make_token_term () in
                                   [([(term, bogus_pos)], (term, bogus_pos))]
                              )}
| _1 = term_match_list
    {                              ( List.rev _1 )}

opt_term_matches:
  
    {                              ( [] )}
| _1 = term_match_list
    {                              ( List.rev _1 )}

term_match_list:
  _1 = term_match_list _2 = TokPipe _3 = term_match
    {                              ( _3 :: _1 )}
| _1 = term_match
    {                              ( [_1] )}

term_match:
  _1 = froms _2 = TokArrow _3 = simple_term
    {                              ( _1, _3 )}

froms:
  _1 = from_list
    {                              ( List.rev _1 )}

from_list:
  _1 = from_list _2 = from
    {                              ( _2 :: _1 )}
| _1 = from
    {                              ( [_1] )}

from:
  _1 = simple_term
    {                              ( _1 )}
| _1 = identifier
    {                              ( pho_make_token_term (), snd _1 )}

quoted_identifier:
  _1 = TokQuotedId
    {                              ( _1 )}

simple_term:
  _1 = module_identifier _2 = opt_term_params _3 = subterms
    {                              ( pho_make_term (fst _1) _2 (fst _3), union_pos (snd _1) (snd _3) )}
| _1 = module_identifier _2 = term_params
    {                              ( pho_make_term (fst _1) (fst _2) [], union_pos (snd _1) (snd _2) )}
| _1 = quoted_identifier
    {                              ( pho_make_var_term _1, snd _1 )}
| _1 = TokInt
    {                              ( pho_make_number_term _1, snd _1 )}
| _1 = TokQuestionMark
    {                              ( pho_make_unique_var_term ("?", _1), _1 )}

module_identifier:
  _1 = modules_rev
    {                              ( _1 )}

modules_rev:
  _1 = identifier
    {                              ( [_1], snd _1 )}
| _1 = modules_rev _2 = TokBang _3 = identifier
    {                              ( _3 :: fst _1, union_pos (snd _1) (snd _3) )}

subterms:
  _1 = TokLeftBrace _2 = opt_subterm_list_semi _3 = TokRightBrace
    {                              ( _2, union_pos _1 _3 )}

opt_subterm_list_semi:
  
    {                              ( [] )}
| _1 = subterms_with_semi
    {                              ( _1 )}

subterms_with_semi:
  _1 = subterm_list_with_semi
    {                              ( List.rev _1 )}

subterm_list_with_semi:
  _1 = subterm_list_with_semi _2 = TokSemi _3 = sub_term
    {                              ( _3 :: _1 )}
| _1 = sub_term
    {                              ( [_1] )}

sub_term:
  _1 = simple_term
    {                              ( pho_make_bterm [] _1 )}
| _1 = identifiers_with_comma _2 = TokDot _3 = simple_term
    {                              ( pho_make_bterm _1 _3 )}

opt_term_params:
  
    {                              ( [] )}
| _1 = term_params
    {                              ( List.rev (fst _1) )}

term_params:
  _1 = TokLeftBrack _2 = term_param_list _3 = TokRightBrack
    {                              ( List.rev _2, union_pos _1 _3 )}

term_param_list:
  _1 = term_param_list _2 = term_param
    {                              ( _2 :: _1 )}
| _1 = term_param
    {                              ( [_1] )}

param_type_id:
  _1 = TokId
    {                              ( match fst _1 with
                                   "s" -> TyString, snd _1
                                 | "n" -> TyNum, snd _1
                                 | "v" -> TyVar, snd _1
                                 | "t" -> TyToken, snd _1
                                 | "l" -> TyLevel, snd _1
                                 | _ ->
                                       raise (ParseError (snd _1, "unknown parameter type"))
                              )}

term_param:
  _1 = TokId _2 = TokColon _3 = TokId
    {                              ( match fst _3 with
                                   "s" ->
                                       make_param (MString (fst _1))
                                 | "n" ->
                                       make_param (MNumber (fst _1))
                                 | "v" ->
                                       make_param (MVar (fst _1))
                                 | "t" ->
                                       make_param (MToken (fst _1))
                                 | _ ->
                                       raise (ParseError (snd _3, "unknown meta-parameter type"))
                              )}
| _1 = TokInt _2 = TokColon _3 = param_type_id
    {                              ( match fst _3 with
                                   TyNum ->
                                       make_param (Number (num_of_int (fst _1)))
                                 | _ ->
                                       raise (ParseError (snd _3, "invalid parameter type"))
                              )}
| _1 = TokString _2 = TokColon _3 = param_type_id
    {                              ( match fst _3 with
                                   TyString ->
                                       make_param (String (fst _1))
                                 | TyToken ->
                                       make_param (Token (fst _1))
                                 | TyVar ->
                                       make_param (Var (fst _1))
                                 | _ ->
                                       raise (ParseError (snd _3, "invalid parameter type"))
                              )}
| _1 = TokString
    {                              ( make_param (String (fst _1)) )}
| _1 = TokInt
    {                              ( make_param (Number (num_of_int (fst _1))) )}
| _1 = TokId
    {                              ( make_param (MString (fst _1)) )}

opt_term_declarations:
  
    {                              ( [] )}
| _1 = term_declaration_list
    {                              ( _1 )}

term_declaration_list:
  _1 = term_declaration
    {                              ( _1 )}
| _1 = term_declaration_list _2 = term_declaration
    {                              ( _2 @ _1 )}

term_declaration:
  _1 = TokDeclare _2 = syntax_terms
    {                              ( _2 )}

syntax_terms:
  _1 = syntax_term_list_rev
    {                              ( List.rev _1 )}

syntax_term_list_rev:
  _1 = syntax_term
    {                              ( [_1] )}
| _1 = syntax_term_list_rev _2 = TokComma _3 = syntax_term
    {                              ( _3 :: _1 )}

syntax_term:
  _1 = st_identifier
    {                              ( _1, 0 )}
| _1 = st_identifier _2 = TokLeftBrace _3 = opt_syntax_subterms _4 = TokRightBrace
    {                              ( _1, List.length _3 )}

st_identifier:
  _1 = identifier
    {                              ( _1 )}
| _1 = identifier _2 = TokLeftBrack _3 = param_type_id _4 = TokRightBrack
    {                              ( _1 )}

opt_syntax_subterms:
  
    {                              ( [] )}
| _1 = syntax_subterms
    {                              ( _1 )}

syntax_subterms:
  _1 = syntax_subterm_list
    {                              ( List.rev _1 )}

syntax_subterm_list:
  _1 = syntax_subterm
    {                              ( [_1] )}
| _1 = syntax_subterm_list _2 = TokSemi _3 = syntax_subterm
    {                              ( _3 :: _1 )}

syntax_subterm:
  _1 = syntax_subterm_base
    {                              ( _1 )}
| _1 = identifier _2 = TokDot _3 = syntax_subterm_base
    {                              ( _3 )}

syntax_subterm_base:
  _1 = quoted_identifier
    {                              ( _1 )}
| _1 = quoted_identifier _2 = TokLeftBrack _3 = quoted_identifier _4 = TokRightBrack
    {                              ( _1 )}

opt_rewrites_section:
  
    {                              ( [] )}
| _1 = rev_rewrites_section_list
    {                              ( List.rev _1 )}

rev_rewrites_section_list:
  _1 = rewrites_section
    {                              ( [_1] )}
| _1 = rev_rewrites_section_list _2 = rewrites_section
    {                              ( _2 :: _1 )}

rewrites_section:
  _1 = TokRewrites _2 = TokLeftBrace _3 = new_rewrites _4 = TokRightBrace
    {                              ( _3 )}

opt_inline_forms:
  
    {                              ( [] )}
| _1 = inline_form_list_rev
    {                              ( List.rev _1 )}

inline_form_list_rev:
  _1 = inline_form
    {                              ( [_1] )}
| _1 = inline_form_list_rev _2 = inline_form
    {                              ( _2 :: _1 )}

inline_form:
  _1 = TokInline _2 = simple_term
    {                              ( _2 )}

%%
